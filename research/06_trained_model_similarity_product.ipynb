{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db49c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee4082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad43464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Embedding model (backbone + embedding head)\n",
    "class ProductEmbeddingModel(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_v2_s', embedding_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained backbone\n",
    "        if backbone_name == 'efficientnet_v2_s':\n",
    "            backbone = models.efficientnet_v2_s(pretrained=True)\n",
    "            backbone_features = backbone.classifier[1].in_features\n",
    "            backbone.classifier = nn.Identity()  # Remove classifier\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = models.resnet50(pretrained=True)\n",
    "            backbone_features = backbone.fc.in_features\n",
    "            backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Embedding head\n",
    "        self.embedding_head = nn.Sequential(\n",
    "            nn.Linear(backbone_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding_head(features)\n",
    "        # L2 normalize embeddings\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb5b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Load trained model and get embeddings\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_trained_model(checkpoint_path='models/product_embedding_model.pth', device=None):\n",
    "    \"\"\"Load the trained embedding model from checkpoint\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    model = ProductEmbeddingModel(\n",
    "        backbone_name=checkpoint['backbone'],\n",
    "        embedding_dim=checkpoint['embedding_dim']\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Loaded model from {checkpoint_path}\")\n",
    "    print(f\"Backbone: {checkpoint['backbone']}\")\n",
    "    print(f\"Embedding dim: {checkpoint['embedding_dim']}\")\n",
    "    print(f\"Trained for {checkpoint['epoch']+1} epochs\")\n",
    "    print(f\"Best loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return model, checkpoint, device\n",
    "\n",
    "def get_image_transform(image_size=224):\n",
    "    \"\"\"Get the same transform used during training (without augmentation)\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_embedding_from_image(model, image_path_or_pil, transform, device):\n",
    "    \"\"\"\n",
    "    Get embedding for a single image\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ProductEmbeddingModel\n",
    "        image_path_or_pil: Path to image (str) or PIL Image\n",
    "        transform: Image transform function\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        embedding: numpy array of shape (embedding_dim,)\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        image = Image.open(image_path_or_pil).convert('RGB')\n",
    "    else:\n",
    "        image = image_path_or_pil.convert('RGB')\n",
    "    \n",
    "    # Transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "        embedding = embedding.cpu().numpy().flatten()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def get_embeddings_batch(model, image_paths, transform, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Get embeddings for multiple images efficiently\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ProductEmbeddingModel\n",
    "        image_paths: List of image paths or PIL Images\n",
    "        transform: Image transform function\n",
    "        device: torch device\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array of shape (num_images, embedding_dim)\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            if isinstance(img_path, str):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            else:\n",
    "                image = img_path.convert('RGB')\n",
    "            batch_images.append(transform(image))\n",
    "        \n",
    "        # Stack into batch tensor\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_tensor)\n",
    "            embeddings_list.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40071825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from models/product_embedding_model_unfrozen2.pth\n",
      "Backbone: efficientnet_v2_s\n",
      "Embedding dim: 1024\n",
      "Trained for 19 epochs\n",
      "Best loss: 0.0057\n",
      "Embedding shape: (1024,)\n",
      "Embedding norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model, checkpoint, device = load_trained_model('models/product_embedding_model_unfrozen2.pth')\n",
    "\n",
    "# Get transform\n",
    "transform = get_image_transform(image_size=224)\n",
    "\n",
    "# Example 1: Get embedding for a single image\n",
    "image_path = 'data/test_images/cropped_image_3.jpg'\n",
    "embedding = get_embedding_from_image(model, image_path, transform, device)\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding norm: {np.linalg.norm(embedding):.4f}\")  # Should be ~1.0 (L2 normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 208 reference images...\n",
      "Built FAISS index with 208 vectors\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS index with all reference images\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "def build_reference_index(model, reference_dir='data/reference_images', \n",
    "                          transform=None, device=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    Build FAISS index from all reference images\n",
    "    \n",
    "    Returns:\n",
    "        index: FAISS index\n",
    "        product_ids: List of product IDs for each embedding\n",
    "        image_paths: List of image paths\n",
    "    \"\"\"\n",
    "    if transform is None:\n",
    "        transform = get_image_transform()\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    reference_path = Path(reference_dir)\n",
    "    all_image_paths = []\n",
    "    product_ids = []\n",
    "    \n",
    "    # Collect all image paths\n",
    "    product_folders = sorted([d for d in reference_path.iterdir() if d.is_dir()])\n",
    "    for product_folder in product_folders:\n",
    "        product_id = product_folder.name\n",
    "        image_files = sorted(product_folder.glob('*.jpg')) + \\\n",
    "                     sorted(product_folder.glob('*.jpeg')) + \\\n",
    "                     sorted(product_folder.glob('*.png'))\n",
    "        \n",
    "        for image_path in image_files:\n",
    "            all_image_paths.append(str(image_path))\n",
    "            product_ids.append(product_id)\n",
    "    \n",
    "    print(f\"Processing {len(all_image_paths)} reference images...\")\n",
    "    \n",
    "    # Get embeddings for all images\n",
    "    embeddings = get_embeddings_batch(model, all_image_paths, transform, device, batch_size)\n",
    "    \n",
    "    # Build FAISS index (using L2 distance since embeddings are normalized)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Convert to float32 for FAISS\n",
    "    embeddings_f32 = embeddings.astype('float32')\n",
    "    index.add(embeddings_f32)\n",
    "    \n",
    "    print(f\"Built FAISS index with {index.ntotal} vectors\")\n",
    "    \n",
    "    return index, product_ids, all_image_paths\n",
    "\n",
    "# Build index\n",
    "index, product_ids, image_paths = build_reference_index(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3af42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_products(query_embedding, index, product_ids, top_k=5, threshold=None):\n",
    "    \"\"\"\n",
    "    Find top-k similar products using L2 distance\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: numpy array of shape (embedding_dim,)\n",
    "        index: FAISS index\n",
    "        product_ids: List of product IDs\n",
    "        top_k: Number of results to return\n",
    "        threshold: Optional maximum L2 distance threshold\n",
    "    \n",
    "    Returns:\n",
    "        results: List of dicts with product_id, distance, rank\n",
    "    \"\"\"\n",
    "    # Reshape for FAISS (needs batch dimension)\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if threshold is None or distance <= threshold:\n",
    "            results.append({\n",
    "                'product_id': product_ids[idx],\n",
    "                'distance': float(distance),\n",
    "                'similarity': float(1 / (1 + distance)),  # Convert distance to similarity\n",
    "                'rank': i + 1,\n",
    "                'image_path': image_paths[idx]\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 similar products:\n",
      "Rank 1: Product 1012 - Distance: 0.5193, Similarity: 0.6582\n",
      "Rank 2: Product 1012 - Distance: 0.5613, Similarity: 0.6405\n",
      "Rank 3: Product 1012 - Distance: 0.6409, Similarity: 0.6094\n",
      "Rank 4: Product 1023 - Distance: 0.6524, Similarity: 0.6052\n",
      "Rank 5: Product 1023 - Distance: 0.7201, Similarity: 0.5814\n"
     ]
    }
   ],
   "source": [
    "# Example: Find similar products\n",
    "query_image = r'data\\test_images\\7.jpg'\n",
    "query_embedding = get_embedding_from_image(model, query_image, transform, device)\n",
    "\n",
    "results = find_similar_products(query_embedding, index, product_ids, top_k=5)\n",
    "print(\"\\nTop 5 similar products:\")\n",
    "for r in results:\n",
    "    print(f\"Rank {r['rank']}: Product {r['product_id']} - Distance: {r['distance']:.4f}, Similarity: {r['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dbe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
