{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b90daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c926ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DINOv2 Model with Fine-tuning Head\n",
    "class FineTunedDINOv2(nn.Module):\n",
    "    def __init__(self, model_name='facebook/dinov2-base', embedding_dim=512, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained DINOv2\n",
    "        self.dinov2 = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Get embedding dimension from DINOv2\n",
    "        if 'base' in model_name:\n",
    "            dinov2_dim = 768\n",
    "        elif 'small' in model_name:\n",
    "            dinov2_dim = 384\n",
    "        elif 'large' in model_name:\n",
    "            dinov2_dim = 1024\n",
    "        else:\n",
    "            dinov2_dim = 768  # default\n",
    "        \n",
    "        # Freeze backbone if needed (for faster training)\n",
    "        if freeze_backbone:\n",
    "            for param in self.dinov2.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Fine-tuning head (projection layer)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(dinov2_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Forward pass through DINOv2\n",
    "        pixel_values: preprocessed images (batch_size, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        # DINOv2 forward pass\n",
    "        outputs = self.dinov2(pixel_values=pixel_values)\n",
    "        \n",
    "        # Get CLS token (first token) - this is the image embedding\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # (batch_size, dinov2_dim)\n",
    "        \n",
    "        # Project to desired embedding dimension\n",
    "        embedding = self.projection_head(cls_token)\n",
    "        \n",
    "        # L2 normalize\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load trained model and get embeddings\n",
    "def load_trained_dinov2_model(checkpoint_path='models/dinov2_finetuned.pth', device=None):\n",
    "    \"\"\"Load the trained DINOv2 model from checkpoint\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    model = FineTunedDINOv2(\n",
    "        model_name=checkpoint['model_name'],\n",
    "        embedding_dim=checkpoint['embedding_dim'],\n",
    "        freeze_backbone=False\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Loaded model from {checkpoint_path}\")\n",
    "    print(f\"Model: {checkpoint['model_name']}\")\n",
    "    print(f\"Embedding dim: {checkpoint['embedding_dim']}\")\n",
    "    print(f\"Trained for {checkpoint['epoch']+1} epochs\")\n",
    "    print(f\"Best loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return model, checkpoint, device\n",
    "\n",
    "def get_image_transform_dinov2(image_size=224):\n",
    "    \"\"\"Get the same transform used during training (without augmentation)\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_embedding_from_image_dinov2(model, image_path_or_pil, transform, device):\n",
    "    \"\"\"\n",
    "    Get embedding for a single image using DINOv2\n",
    "    \n",
    "    Args:\n",
    "        model: Trained FineTunedDINOv2\n",
    "        image_path_or_pil: Path to image (str) or PIL Image\n",
    "        transform: Image transform function\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        embedding: numpy array of shape (embedding_dim,)\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        image = Image.open(image_path_or_pil).convert('RGB')\n",
    "    else:\n",
    "        image = image_path_or_pil.convert('RGB')\n",
    "    \n",
    "    # Transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "        embedding = embedding.cpu().numpy().flatten()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def get_embeddings_batch_dinov2(model, image_paths, transform, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Get embeddings for multiple images efficiently using DINOv2\n",
    "    \n",
    "    Args:\n",
    "        model: Trained FineTunedDINOv2\n",
    "        image_paths: List of image paths or PIL Images\n",
    "        transform: Image transform function\n",
    "        device: torch device\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array of shape (num_images, embedding_dim)\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                if isinstance(img_path, str):\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                else:\n",
    "                    image = img_path.convert('RGB')\n",
    "                batch_images.append(transform(image))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not batch_images:\n",
    "            continue\n",
    "            \n",
    "        # Stack into batch tensor\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_tensor)\n",
    "            embeddings_list.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    if embeddings_list:\n",
    "        embeddings = np.vstack(embeddings_list)\n",
    "    else:\n",
    "        embeddings = np.array([])\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd387907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build FAISS index with all reference images\n",
    "def build_reference_index_dinov2(model, reference_dir='data/reference_images', \n",
    "                                 transform=None, device=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    Build FAISS index from all reference images using DINOv2\n",
    "    \n",
    "    Returns:\n",
    "        index: FAISS index\n",
    "        product_ids: List of product IDs for each embedding\n",
    "        image_paths: List of image paths\n",
    "    \"\"\"\n",
    "    if transform is None:\n",
    "        transform = get_image_transform_dinov2()\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    reference_path = Path(reference_dir)\n",
    "    all_image_paths = []\n",
    "    product_ids = []\n",
    "    \n",
    "    # Collect all image paths\n",
    "    product_folders = sorted([d for d in reference_path.iterdir() if d.is_dir()],\n",
    "                            key=lambda x: int(x.name) if x.name.isdigit() else 0)\n",
    "    \n",
    "    for product_folder in product_folders:\n",
    "        product_id = product_folder.name\n",
    "        image_files = sorted(product_folder.glob('*.jpg')) + \\\n",
    "                     sorted(product_folder.glob('*.jpeg')) + \\\n",
    "                     sorted(product_folder.glob('*.png'))\n",
    "        \n",
    "        for image_path in image_files:\n",
    "            all_image_paths.append(str(image_path))\n",
    "            product_ids.append(product_id)\n",
    "    \n",
    "    print(f\"Processing {len(all_image_paths)} reference images...\")\n",
    "    \n",
    "    # Get embeddings for all images\n",
    "    embeddings = get_embeddings_batch_dinov2(model, all_image_paths, transform, device, batch_size)\n",
    "    \n",
    "    # Build FAISS index (using Inner Product for cosine similarity since embeddings are normalized)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product = cosine similarity for normalized vectors\n",
    "    \n",
    "    # Convert to float32 for FAISS\n",
    "    embeddings_f32 = embeddings.astype('float32')\n",
    "    index.add(embeddings_f32)\n",
    "    \n",
    "    print(f\"Built FAISS index with {index.ntotal} vectors\")\n",
    "    \n",
    "    return index, product_ids, all_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cba4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Find similar products\n",
    "def find_similar_products_dinov2(query_embedding, index, product_ids, image_paths=None, \n",
    "                                 top_k=5, threshold=None):\n",
    "    \"\"\"\n",
    "    Find top-k similar products using cosine similarity (Inner Product)\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: numpy array of shape (embedding_dim,)\n",
    "        index: FAISS index (IndexFlatIP)\n",
    "        product_ids: List of product IDs\n",
    "        image_paths: Optional list of image paths\n",
    "        top_k: Number of results to return\n",
    "        threshold: Optional minimum similarity threshold (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        results: List of dicts with product_id, similarity, rank\n",
    "    \"\"\"\n",
    "    # Reshape for FAISS (needs batch dimension)\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "    \n",
    "    # Search (returns cosine similarity scores, higher = more similar)\n",
    "    similarities, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):\n",
    "        if threshold is None or similarity >= threshold:\n",
    "            result = {\n",
    "                'product_id': product_ids[idx],\n",
    "                'similarity': float(similarity),\n",
    "                'similarity_percent': f\"{similarity*100:.2f}%\",\n",
    "                'rank': i + 1\n",
    "            }\n",
    "            if image_paths:\n",
    "                result['image_path'] = image_paths[idx]\n",
    "            results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0faff228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from models/dinov2_finetuned.pth\n",
      "Model: facebook/dinov2-base\n",
      "Embedding dim: 512\n",
      "Trained for 5 epochs\n",
      "Best loss: 0.2295\n"
     ]
    }
   ],
   "source": [
    "model, checkpoint, device = load_trained_dinov2_model('models/dinov2_finetuned.pth')\n",
    "transform = get_image_transform_dinov2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85c7a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (512,)\n",
      "Embedding norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Example: Get embedding for a single image\n",
    "image_path = 'data/test_images/cropped_image_3.jpg'\n",
    "embedding = get_embedding_from_image_dinov2(model, image_path, transform, device)\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding norm: {np.linalg.norm(embedding):.4f}\")  # Should be ~1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f914d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 166 reference images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 11/11 [00:36<00:00,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built FAISS index with 166 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build FAISS index\n",
    "index, product_ids, image_paths = build_reference_index_dinov2(model, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd9c0f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 15832: Similarity=100.00%\n",
      "Product 675: Similarity=100.00%\n",
      "Product 79: Similarity=100.00%\n",
      "Product 15832: Similarity=100.00%\n",
      "Product 1003: Similarity=99.99%\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Query new image\n",
    "query_image_path = r'data\\test_images\\cropped_image_3.jpg'\n",
    "query_embedding = get_embedding_from_image_dinov2(model, query_image_path, transform, device)\n",
    "\n",
    "# 4. Find matches\n",
    "matches = find_similar_products_dinov2(query_embedding, index, product_ids, image_paths, \n",
    "                                      top_k=5, threshold=0.7)\n",
    "\n",
    "# 5. Display results\n",
    "for match in matches:\n",
    "    print(f\"Product {match['product_id']}: Similarity={match['similarity_percent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a238f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
