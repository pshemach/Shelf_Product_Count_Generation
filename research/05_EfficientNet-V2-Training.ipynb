{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9e26dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73aab2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0265ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset class\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, reference_dir='data/reference_images', \n",
    "                 image_size=224, augment=True):\n",
    "        self.reference_dir = Path(reference_dir)\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load all images with their product IDs\n",
    "        self.images = []\n",
    "        self.product_ids = []\n",
    "        self.product_to_images = {}\n",
    "        \n",
    "        product_folders = sorted([d for d in self.reference_dir.iterdir() if d.is_dir()])\n",
    "        \n",
    "        for product_folder in product_folders:\n",
    "            product_id = product_folder.name\n",
    "            image_files = sorted(product_folder.glob('*.jpg')) + \\\n",
    "                         sorted(product_folder.glob('*.jpeg')) + \\\n",
    "                         sorted(product_folder.glob('*.png'))\n",
    "            \n",
    "            self.product_to_images[product_id] = []\n",
    "            \n",
    "            for image_path in image_files:\n",
    "                self.images.append(str(image_path))\n",
    "                self.product_ids.append(product_id)\n",
    "                self.product_to_images[product_id].append(len(self.images) - 1)\n",
    "        \n",
    "        # Create product ID to integer mapping\n",
    "        unique_products = sorted(set(self.product_ids))\n",
    "        self.product_to_idx = {pid: idx for idx, pid in enumerate(unique_products)}\n",
    "        self.idx_to_product = {idx: pid for pid, idx in self.product_to_idx.items()}\n",
    "        self.num_products = len(unique_products)\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {self.num_products} products\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        product_id = self.product_ids[idx]\n",
    "        product_idx = self.product_to_idx[product_id]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, product_idx, product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "580f91c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 208 images from 68 products\n"
     ]
    }
   ],
   "source": [
    "data = ProductDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8311d36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1000': [0, 1, 2],\n",
       " '1001': [3, 4, 5, 6],\n",
       " '1002': [7, 8, 9, 10],\n",
       " '1003': [11, 12, 13],\n",
       " '1004': [14, 15, 16, 17, 18],\n",
       " '1005': [19, 20, 21],\n",
       " '1006': [22, 23, 24, 25, 26],\n",
       " '1007': [27, 28, 29],\n",
       " '1008': [30, 31],\n",
       " '1009': [32, 33, 34],\n",
       " '1010': [35, 36],\n",
       " '1011': [37, 38],\n",
       " '1012': [39, 40, 41],\n",
       " '1013': [42, 43],\n",
       " '1014': [44, 45],\n",
       " '1015': [46, 47],\n",
       " '1016': [48, 49],\n",
       " '1017': [50, 51],\n",
       " '1018': [52, 53],\n",
       " '1019': [54, 55],\n",
       " '1020': [56, 57],\n",
       " '1021': [58, 59],\n",
       " '1022': [60, 61],\n",
       " '1023': [62, 63],\n",
       " '1024': [64, 65],\n",
       " '1025': [66, 67],\n",
       " '1026': [68, 69],\n",
       " '1027': [70, 71],\n",
       " '1028': [72, 73],\n",
       " '1029': [74, 75, 76],\n",
       " '1030': [77, 78],\n",
       " '1031': [79, 80],\n",
       " '1032': [81, 82, 83, 84],\n",
       " '1033': [85, 86],\n",
       " '1034': [87, 88],\n",
       " '1035': [89, 90],\n",
       " '1036': [91, 92],\n",
       " '1037': [93, 94],\n",
       " '1038': [95, 96],\n",
       " '1039': [97, 98, 99],\n",
       " '15785': [100, 101, 102],\n",
       " '15827': [103, 104, 105],\n",
       " '15829': [106, 107, 108, 109, 110, 111],\n",
       " '15832': [112, 113, 114, 115, 116],\n",
       " '58': [117, 118, 119, 120, 121, 122, 123, 124],\n",
       " '59': [125, 126, 127, 128],\n",
       " '60': [129, 130, 131],\n",
       " '667': [132, 133, 134, 135, 136],\n",
       " '668': [137, 138, 139],\n",
       " '669': [140, 141, 142, 143],\n",
       " '670': [144, 145, 146, 147],\n",
       " '671': [148, 149],\n",
       " '672': [150, 151, 152, 153, 154, 155, 156],\n",
       " '673': [157, 158, 159],\n",
       " '674': [160, 161],\n",
       " '675': [162, 163, 164],\n",
       " '677': [165, 166, 167, 168],\n",
       " '74': [169, 170, 171, 172],\n",
       " '75': [173, 174],\n",
       " '76': [175, 176, 177, 178],\n",
       " '77': [179, 180, 181, 182, 183, 184, 185],\n",
       " '78': [186, 187, 188, 189, 190, 191],\n",
       " '79': [192, 193, 194, 195, 196],\n",
       " '80': [197, 198],\n",
       " '89': [199, 200],\n",
       " '9': [201, 202],\n",
       " '90': [203, 204],\n",
       " '91': [205, 206, 207]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.product_to_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4ed6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Embedding model (backbone + embedding head)\n",
    "class ProductEmbeddingModel(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_v2_s', embedding_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained backbone\n",
    "        if backbone_name == 'efficientnet_v2_s':\n",
    "            backbone = models.efficientnet_v2_s(pretrained=True)\n",
    "            backbone_features = backbone.classifier[1].in_features\n",
    "            backbone.classifier = nn.Identity()  # Remove classifier\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = models.resnet50(pretrained=True)\n",
    "            backbone_features = backbone.fc.in_features\n",
    "            backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Embedding head\n",
    "        self.embedding_head = nn.Sequential(\n",
    "            nn.Linear(backbone_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding_head(features)\n",
    "        # L2 normalize embeddings\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ad8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "backbone = models.efficientnet_v2_s(pretrained=True)\n",
    "backbone.classifier = nn.Identity()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "128f5955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.005, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.01, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)\n",
       "      )\n",
       "      (2): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
       "      )\n",
       "      (3): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.035, mode=row)\n",
       "      )\n",
       "      (2): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
       "      )\n",
       "      (3): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.045, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.065, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.075, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.085, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.095, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)\n",
       "      )\n",
       "      (6): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)\n",
       "      )\n",
       "      (7): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)\n",
       "      )\n",
       "      (8): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.135, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15, mode=row)\n",
       "      )\n",
       "      (6): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.155, mode=row)\n",
       "      )\n",
       "      (7): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "      )\n",
       "      (8): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.165, mode=row)\n",
       "      )\n",
       "      (9): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17, mode=row)\n",
       "      )\n",
       "      (10): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.175, mode=row)\n",
       "      )\n",
       "      (11): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
       "      )\n",
       "      (12): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.185, mode=row)\n",
       "      )\n",
       "      (13): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19, mode=row)\n",
       "      )\n",
       "      (14): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.195, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Identity()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1bbf26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dNormActivation(\n",
       "  (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(backbone.features.children())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3bf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Triplet Loss (metric learning)\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"\n",
    "        anchor: embedding of anchor image\n",
    "        positive: embedding of positive (same product) image\n",
    "        negative: embedding of negative (different product) image\n",
    "        \"\"\"\n",
    "        distance_positive = nn.functional.pairwise_distance(anchor, positive)\n",
    "        distance_negative = nn.functional.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7e891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Triplet sampler (creates triplets)\n",
    "class TripletSampler:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.product_to_indices = {}\n",
    "        \n",
    "        for idx, product_id in enumerate(dataset.product_ids):\n",
    "            if product_id not in self.product_to_indices:\n",
    "                self.product_to_indices[product_id] = []\n",
    "            self.product_to_indices[product_id].append(idx)\n",
    "    \n",
    "    def sample_triplet(self):\n",
    "        \"\"\"Sample anchor, positive, negative\"\"\"\n",
    "        # Random anchor product\n",
    "        anchor_product = random.choice(list(self.product_to_indices.keys()))\n",
    "        anchor_idx, positive_idx = random.sample(\n",
    "            self.product_to_indices[anchor_product], 2\n",
    "        )\n",
    "        \n",
    "        # Random negative product (different from anchor)\n",
    "        negative_product = random.choice([\n",
    "            p for p in self.product_to_indices.keys() \n",
    "            if p != anchor_product\n",
    "        ])\n",
    "        negative_idx = random.choice(self.product_to_indices[negative_product])\n",
    "        \n",
    "        return anchor_idx, positive_idx, negative_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e38298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Training function\n",
    "# def train_embedding_model(\n",
    "#     reference_dir='data/reference_images',\n",
    "#     backbone='efficientnet_v2_s',\n",
    "#     embedding_dim=512,\n",
    "#     batch_size=32,\n",
    "#     num_epochs=50,\n",
    "#     learning_rate=0.001,\n",
    "#     margin=0.5,\n",
    "#     save_path='models/product_embedding_model.pth'\n",
    "# ):\n",
    "    \n",
    "#     # Device\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     # Dataset\n",
    "#     train_dataset = ProductDataset(reference_dir, augment=True)\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=True,\n",
    "#         num_workers=0,  # Use 0 for Windows/Jupyter\n",
    "#         pin_memory=False\n",
    "#     )\n",
    "    \n",
    "#     # Model\n",
    "#     model = ProductEmbeddingModel(backbone, embedding_dim)\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Loss and optimizer\n",
    "#     criterion = TripletLoss(margin=margin)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "#     # Triplet sampler\n",
    "#     triplet_sampler = TripletSampler(train_dataset)\n",
    "    \n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     best_loss = float('inf')\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0\n",
    "#         num_batches = 0\n",
    "        \n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "#         for batch_images, batch_labels, batch_product_ids in progress_bar:\n",
    "#             # Sample triplets\n",
    "#             anchor_indices = []\n",
    "#             positive_indices = []\n",
    "#             negative_indices = []\n",
    "            \n",
    "#             for _ in range(batch_size):\n",
    "#                 a, p, n = triplet_sampler.sample_triplet()\n",
    "#                 anchor_indices.append(a)\n",
    "#                 positive_indices.append(p)\n",
    "#                 negative_indices.append(n)\n",
    "            \n",
    "#             # Get images - access dataset directly (safe with num_workers=0)\n",
    "#             anchor_images = torch.stack([train_dataset[i][0] for i in anchor_indices]).to(device)\n",
    "#             positive_images = torch.stack([train_dataset[i][0] for i in positive_indices]).to(device)\n",
    "#             negative_images = torch.stack([train_dataset[i][0] for i in negative_indices]).to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             anchor_emb = model(anchor_images)\n",
    "#             positive_emb = model(positive_images)\n",
    "#             negative_emb = model(negative_images)\n",
    "            \n",
    "#             # Loss\n",
    "#             loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "#             # Backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "#         avg_loss = epoch_loss / num_batches\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "#         # Save best model\n",
    "#         if avg_loss < best_loss:\n",
    "#             best_loss = avg_loss\n",
    "#             torch.save({\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'backbone': backbone,\n",
    "#                 'embedding_dim': embedding_dim,\n",
    "#                 'num_products': train_dataset.num_products,\n",
    "#                 'product_to_idx': train_dataset.product_to_idx,\n",
    "#                 'epoch': epoch,\n",
    "#                 'loss': avg_loss\n",
    "#             }, save_path)\n",
    "#             print(f\"Saved best model (loss: {avg_loss:.4f})\")\n",
    "    \n",
    "#     print(f\"Training complete! Best loss: {best_loss:.4f}\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a448cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Usage: Train the model\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = train_embedding_model(\n",
    "#         reference_dir='data/reference_images',\n",
    "#         backbone='efficientnet_v2_s',\n",
    "#         embedding_dim=512,\n",
    "#         batch_size=16,  # Adjust based on GPU memory\n",
    "#         num_epochs=10,\n",
    "#         learning_rate=0.001,\n",
    "#         margin=0.5\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33b9bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Training function with checkpoint resuming support\n",
    "# def train_embedding_model(\n",
    "#     reference_dir='data/reference_images',\n",
    "#     backbone='efficientnet_v2_s',\n",
    "#     embedding_dim=512,\n",
    "#     batch_size=32,\n",
    "#     num_epochs=50,\n",
    "#     learning_rate=0.001,\n",
    "#     margin=0.5,\n",
    "#     save_path='models/product_embedding_model.pth',\n",
    "#     resume_from_checkpoint=None,  # Path to checkpoint to resume from, or None to start fresh\n",
    "#     start_epoch=0  # Will be updated if resuming\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Train embedding model with optional checkpoint resuming\n",
    "    \n",
    "#     Args:\n",
    "#         resume_from_checkpoint: Path to checkpoint file to resume from.\n",
    "#                                If None, starts training from scratch.\n",
    "#                                If same as save_path, automatically resumes if file exists.\n",
    "#     \"\"\"\n",
    "#     # Device\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     # Dataset\n",
    "#     train_dataset = ProductDataset(reference_dir, augment=True)\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=True,\n",
    "#         num_workers=0,  # Use 0 for Windows/Jupyter\n",
    "#         pin_memory=False\n",
    "#     )\n",
    "    \n",
    "#     # Model\n",
    "#     model = ProductEmbeddingModel(backbone, embedding_dim)\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Loss and optimizer\n",
    "#     criterion = TripletLoss(margin=margin)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "#     # Checkpoint resuming logic\n",
    "#     best_loss = float('inf')\n",
    "#     start_epoch = 0\n",
    "    \n",
    "#     # Determine checkpoint path\n",
    "#     checkpoint_path = resume_from_checkpoint if resume_from_checkpoint else save_path\n",
    "    \n",
    "#     # Try to load checkpoint\n",
    "#     if Path(checkpoint_path).exists():\n",
    "#         print(f\"\\nFound checkpoint at {checkpoint_path}\")\n",
    "#         print(\"Loading checkpoint to resume training...\")\n",
    "        \n",
    "#         checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "#         # Load model state\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         # Load optimizer state (if available)\n",
    "#         if 'optimizer_state_dict' in checkpoint:\n",
    "#             optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#             print(\"Loaded optimizer state\")\n",
    "        \n",
    "#         # Load scheduler state (if available)\n",
    "#         if 'scheduler_state_dict' in checkpoint:\n",
    "#             scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#             print(\"Loaded scheduler state\")\n",
    "        \n",
    "#         # Resume from saved epoch\n",
    "#         if 'epoch' in checkpoint:\n",
    "#             start_epoch = checkpoint['epoch'] + 1\n",
    "#             print(f\"Resuming from epoch {start_epoch}\")\n",
    "        \n",
    "#         # Resume best loss\n",
    "#         if 'loss' in checkpoint:\n",
    "#             best_loss = checkpoint['loss']\n",
    "#             print(f\"Previous best loss: {best_loss:.4f}\")\n",
    "        \n",
    "#         # Verify architecture matches\n",
    "#         if checkpoint.get('backbone') != backbone:\n",
    "#             print(f\"Warning: Backbone mismatch! Checkpoint: {checkpoint.get('backbone')}, Current: {backbone}\")\n",
    "#         if checkpoint.get('embedding_dim') != embedding_dim:\n",
    "#             print(f\"Warning: Embedding dim mismatch! Checkpoint: {checkpoint.get('embedding_dim')}, Current: {embedding_dim}\")\n",
    "        \n",
    "#         print(\"Checkpoint loaded successfully!\\n\")\n",
    "#     else:\n",
    "#         print(f\"No checkpoint found at {checkpoint_path}. Starting training from scratch.\\n\")\n",
    "    \n",
    "#     # Triplet sampler\n",
    "#     triplet_sampler = TripletSampler(train_dataset)\n",
    "    \n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "    \n",
    "#     for epoch in range(start_epoch, num_epochs):\n",
    "#         epoch_loss = 0\n",
    "#         num_batches = 0\n",
    "        \n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "#         for batch_images, batch_labels, batch_product_ids in progress_bar:\n",
    "#             # Sample triplets\n",
    "#             anchor_indices = []\n",
    "#             positive_indices = []\n",
    "#             negative_indices = []\n",
    "            \n",
    "#             for _ in range(batch_size):\n",
    "#                 a, p, n = triplet_sampler.sample_triplet()\n",
    "#                 anchor_indices.append(a)\n",
    "#                 positive_indices.append(p)\n",
    "#                 negative_indices.append(n)\n",
    "            \n",
    "#             # Get images - access dataset directly (safe with num_workers=0)\n",
    "#             anchor_images = torch.stack([train_dataset[i][0] for i in anchor_indices]).to(device)\n",
    "#             positive_images = torch.stack([train_dataset[i][0] for i in positive_indices]).to(device)\n",
    "#             negative_images = torch.stack([train_dataset[i][0] for i in negative_indices]).to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             anchor_emb = model(anchor_images)\n",
    "#             positive_emb = model(positive_images)\n",
    "#             negative_emb = model(negative_images)\n",
    "            \n",
    "#             # Loss\n",
    "#             loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "#             # Backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "#         avg_loss = epoch_loss / num_batches\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         current_lr = scheduler.get_last_lr()[0]\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - LR: {current_lr:.6f}\")\n",
    "        \n",
    "#         # Save best model (with full training state)\n",
    "#         if avg_loss < best_loss:\n",
    "#             best_loss = avg_loss\n",
    "#             torch.save({\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                 'backbone': backbone,\n",
    "#                 'embedding_dim': embedding_dim,\n",
    "#                 'num_products': train_dataset.num_products,\n",
    "#                 'product_to_idx': train_dataset.product_to_idx,\n",
    "#                 'epoch': epoch,\n",
    "#                 'loss': avg_loss,\n",
    "#                 'learning_rate': current_lr\n",
    "#             }, save_path)\n",
    "#             print(f\"Saved best model (loss: {avg_loss:.4f})\")\n",
    "    \n",
    "#     print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aac28ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explicitly specify checkpoint to resume from\n",
    "# model = train_embedding_model(\n",
    "#     reference_dir='data/reference_images',\n",
    "#     backbone='efficientnet_v2_s',\n",
    "#     embedding_dim=512,\n",
    "#     batch_size=16,\n",
    "#     num_epochs=30,  # Train for 30 more epochs\n",
    "#     learning_rate=0.0005,  # Can adjust learning rate\n",
    "#     margin=0.5,\n",
    "#     save_path='models/product_embedding_model_v2.pth',\n",
    "#     resume_from_checkpoint='models/product_embedding_model.pth'  # Resume from previous model\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1560677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_embedding_model(\n",
    "#     reference_dir='data/reference_images',\n",
    "#     backbone='efficientnet_v2_s',\n",
    "#     embedding_dim=1024,  # Changed from 512 to 1024\n",
    "#     batch_size=16,\n",
    "#     num_epochs=20,\n",
    "#     learning_rate=0.001,\n",
    "#     margin=0.5,\n",
    "#     save_path='models/product_embedding_model_1024.pth'  # Use different name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d092d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training function with best + last model saving and auto-resume from last\n",
    "def train_embedding_model(\n",
    "    reference_dir='data/reference_images',\n",
    "    backbone='efficientnet_v2_s',\n",
    "    embedding_dim=512,\n",
    "    batch_size=32,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    margin=0.5,\n",
    "    save_path='models/product_embedding_model.pth',\n",
    "    resume_from_checkpoint=None,  # Path to checkpoint to resume from, or None for auto-resume\n",
    "    resume_from_best=False  # If True, resume from best model instead of last\n",
    "):\n",
    "    \"\"\"\n",
    "    Train embedding model with best and last model saving\n",
    "    \n",
    "    Args:\n",
    "        save_path: Base path for saving models. Will create:\n",
    "                  - {save_path} -> best model\n",
    "                  - {save_path.replace('.pth', '_last.pth')} -> last model\n",
    "        resume_from_checkpoint: Explicit checkpoint path, or None for auto-resume\n",
    "        resume_from_best: If True and resume_from_checkpoint is None, resume from best model\n",
    "                          Otherwise, resumes from last model (default)\n",
    "    \"\"\"\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Determine model paths\n",
    "    best_model_path = save_path\n",
    "    last_model_path = save_path.replace('.pth', '_last.pth')\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = ProductDataset(reference_dir, augment=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Use 0 for Windows/Jupyter\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = ProductEmbeddingModel(backbone, embedding_dim)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = TripletLoss(margin=margin)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    # Checkpoint resuming logic\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Determine which checkpoint to load\n",
    "    if resume_from_checkpoint is not None:\n",
    "        # Explicit checkpoint path provided\n",
    "        checkpoint_path = resume_from_checkpoint\n",
    "        checkpoint_type = \"explicit\"\n",
    "    elif resume_from_best and Path(best_model_path).exists():\n",
    "        # Resume from best model\n",
    "        checkpoint_path = best_model_path\n",
    "        checkpoint_type = \"best\"\n",
    "    elif Path(last_model_path).exists():\n",
    "        # Auto-resume from last model (default)\n",
    "        checkpoint_path = last_model_path\n",
    "        checkpoint_type = \"last\"\n",
    "    else:\n",
    "        # No checkpoint found\n",
    "        checkpoint_path = None\n",
    "        checkpoint_type = None\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        print(f\"\\nFound {checkpoint_type} checkpoint at {checkpoint_path}\")\n",
    "        print(\"Loading checkpoint to resume training...\")\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer state (if available)\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"Loaded optimizer state\")\n",
    "        \n",
    "        # Load scheduler state (if available)\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(\"Loaded scheduler state\")\n",
    "        \n",
    "        # Resume from saved epoch\n",
    "        if 'epoch' in checkpoint:\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "        \n",
    "        # Resume best loss\n",
    "        if 'loss' in checkpoint:\n",
    "            best_loss = checkpoint['loss']\n",
    "            print(f\"Previous best loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Verify architecture matches\n",
    "        if checkpoint.get('backbone') != backbone:\n",
    "            print(f\"Warning: Backbone mismatch! Checkpoint: {checkpoint.get('backbone')}, Current: {backbone}\")\n",
    "        if checkpoint.get('embedding_dim') != embedding_dim:\n",
    "            print(f\"Warning: Embedding dim mismatch! Checkpoint: {checkpoint.get('embedding_dim')}, Current: {embedding_dim}\")\n",
    "        \n",
    "        print(\"Checkpoint loaded successfully!\\n\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found. Starting training from scratch.\\n\")\n",
    "    \n",
    "    # Triplet sampler\n",
    "    triplet_sampler = TripletSampler(train_dataset)\n",
    "    \n",
    "    # Helper function to save checkpoint\n",
    "    def save_checkpoint(epoch, loss, val_loss=None, is_best=False, is_last=False):\n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'backbone': backbone,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'num_products': train_dataset.num_products,\n",
    "            'product_to_idx': train_dataset.product_to_idx,\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'learning_rate': scheduler.get_last_lr()[0]\n",
    "        }\n",
    "        if val_loss is not None:\n",
    "            checkpoint_data['val_loss'] = val_loss\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save(checkpoint_data, best_model_path)\n",
    "            print(f\"Saved best model (loss: {loss:.4f})\")\n",
    "        \n",
    "        if is_last:\n",
    "            torch.save(checkpoint_data, last_model_path)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_images, batch_labels, batch_product_ids in progress_bar:\n",
    "            # Sample triplets\n",
    "            anchor_indices = []\n",
    "            positive_indices = []\n",
    "            negative_indices = []\n",
    "            \n",
    "            for _ in range(batch_size):\n",
    "                a, p, n = triplet_sampler.sample_triplet()\n",
    "                anchor_indices.append(a)\n",
    "                positive_indices.append(p)\n",
    "                negative_indices.append(n)\n",
    "            \n",
    "            # Get images - access dataset directly (safe with num_workers=0)\n",
    "            anchor_images = torch.stack([train_dataset[i][0] for i in anchor_indices]).to(device)\n",
    "            positive_images = torch.stack([train_dataset[i][0] for i in positive_indices]).to(device)\n",
    "            negative_images = torch.stack([train_dataset[i][0] for i in negative_indices]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            anchor_emb = model(anchor_images)\n",
    "            positive_emb = model(positive_images)\n",
    "            negative_emb = model(negative_images)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        is_best = avg_loss < best_loss\n",
    "        if is_best:\n",
    "            best_loss = avg_loss\n",
    "        \n",
    "        # Save checkpoints\n",
    "        save_checkpoint(epoch, avg_loss, is_best=is_best, is_last=True)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - LR: {current_lr:.6f}\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "    print(f\"Last model saved at: {last_model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0cd934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 208 images from 68 products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting training from scratch.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|| 13/13 [01:59<00:00,  9.18s/it, loss=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.1434)\n",
      "Epoch 1/15 - Loss: 0.1434 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|| 13/13 [02:10<00:00, 10.07s/it, loss=0.0312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0584)\n",
      "Epoch 2/15 - Loss: 0.0584 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|| 13/13 [02:26<00:00, 11.26s/it, loss=0.0478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Loss: 0.0623 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|| 13/13 [02:20<00:00, 10.84s/it, loss=0.1446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Loss: 0.1101 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|| 13/13 [02:31<00:00, 11.67s/it, loss=0.2199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Loss: 0.1547 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|| 13/13 [02:39<00:00, 12.24s/it, loss=0.1711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Loss: 0.1278 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|| 13/13 [02:23<00:00, 11.07s/it, loss=0.1972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Loss: 0.1336 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|| 13/13 [02:21<00:00, 10.87s/it, loss=0.1030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Loss: 0.1098 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|| 13/13 [02:19<00:00, 10.72s/it, loss=0.0932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Loss: 0.1211 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|| 13/13 [02:19<00:00, 10.70s/it, loss=0.1321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Loss: 0.1324 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|| 13/13 [02:18<00:00, 10.69s/it, loss=0.1422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Loss: 0.0991 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|| 13/13 [02:17<00:00, 10.57s/it, loss=0.0950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Loss: 0.1046 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|| 13/13 [02:18<00:00, 10.67s/it, loss=0.1600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Loss: 0.1406 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|| 13/13 [02:16<00:00, 10.50s/it, loss=0.0672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Loss: 0.1119 - LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|| 13/13 [02:19<00:00, 10.72s/it, loss=0.1985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Loss: 0.1435 - LR: 0.001000\n",
      "\n",
      "Training complete! Best loss: 0.0584\n",
      "Best model saved at: models/product_embedding_model.pth\n",
      "Last model saved at: models/product_embedding_model_last.pth\n"
     ]
    }
   ],
   "source": [
    "model = train_embedding_model(\n",
    "    reference_dir='data/reference_images',\n",
    "    backbone='efficientnet_v2_s',\n",
    "    embedding_dim=1024,\n",
    "    batch_size=16,\n",
    "    num_epochs=15,\n",
    "    learning_rate=0.001,\n",
    "    margin=0.5,\n",
    "    save_path='models/product_embedding_model.pth'\n",
    "    # Will create:\n",
    "    # - models/product_embedding_model.pth (best model)\n",
    "    # - models/product_embedding_model_last.pth (last model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b6fe03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Embedding model with layer freezing support\n",
    "class ProductEmbeddingModel(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_v2_s', embedding_dim=512,\n",
    "                 freeze_backbone=True, unfreeze_last_n_layers=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained backbone\n",
    "        if backbone_name == 'efficientnet_v2_s':\n",
    "            backbone = models.efficientnet_v2_s(pretrained=True)\n",
    "            backbone_features = backbone.classifier[1].in_features\n",
    "            backbone.classifier = nn.Identity()  # Remove classifier\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = models.resnet50(pretrained=True)\n",
    "            backbone_features = backbone.fc.in_features\n",
    "            backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        # Setup backbone freezing\n",
    "        self._setup_backbone_freezing(freeze_backbone, unfreeze_last_n_layers)\n",
    "        \n",
    "        # Embedding head\n",
    "        self.embedding_head = nn.Sequential(\n",
    "            nn.Linear(backbone_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def _setup_backbone_freezing(self, freeze_backbone, unfreeze_last_n_layers):\n",
    "        \"\"\"Setup which backbone layers to freeze/unfreeze\"\"\"\n",
    "        if not freeze_backbone:\n",
    "            # Don't freeze anything\n",
    "            print(\"Backbone: Fully trainable\")\n",
    "            return\n",
    "        \n",
    "        if unfreeze_last_n_layers == 0:\n",
    "            # Freeze entire backbone\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Backbone: Fully frozen\")\n",
    "        else:\n",
    "            # Freeze all layers first\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            # Unfreeze last N layers\n",
    "            if self.backbone_name == 'efficientnet_v2_s':\n",
    "                # EfficientNet V2 S structure: features -> avgpool -> classifier (removed)\n",
    "                # features contains multiple blocks\n",
    "                blocks = list(self.backbone.features.children())\n",
    "                total_blocks = len(blocks)\n",
    "                \n",
    "                # Unfreeze last N blocks\n",
    "                layers_to_unfreeze = min(unfreeze_last_n_layers, total_blocks)\n",
    "                for i in range(total_blocks - layers_to_unfreeze, total_blocks):\n",
    "                    for param in blocks[i].parameters():\n",
    "                        param.requires_grad = True\n",
    "                \n",
    "                print(f\"Backbone: Unfroze last {layers_to_unfreeze} feature blocks (out of {total_blocks})\")\n",
    "                \n",
    "            elif self.backbone_name == 'resnet50':\n",
    "                # ResNet structure: conv1, bn1, relu, maxpool, layer1-4, avgpool, fc (removed)\n",
    "                if unfreeze_last_n_layers >= 1:\n",
    "                    for param in self.backbone.layer4.parameters():\n",
    "                        param.requires_grad = True\n",
    "                if unfreeze_last_n_layers >= 2:\n",
    "                    for param in self.backbone.layer3.parameters():\n",
    "                        param.requires_grad = True\n",
    "                if unfreeze_last_n_layers >= 3:\n",
    "                    for param in self.backbone.layer2.parameters():\n",
    "                        param.requires_grad = True\n",
    "                if unfreeze_last_n_layers >= 4:\n",
    "                    for param in self.backbone.layer1.parameters():\n",
    "                        param.requires_grad = True\n",
    "                \n",
    "                print(f\"Backbone: Unfroze last {unfreeze_last_n_layers} ResNet layers\")\n",
    "    \n",
    "    def print_trainable_params(self):\n",
    "        \"\"\"Print summary of trainable vs frozen parameters\"\"\"\n",
    "        total_params = 0\n",
    "        trainable_params = 0\n",
    "        frozen_params = 0\n",
    "        \n",
    "        backbone_trainable = 0\n",
    "        head_trainable = 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL PARAMETER SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            total_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                if 'backbone' in name:\n",
    "                    backbone_trainable += param.numel()\n",
    "                else:\n",
    "                    head_trainable += param.numel()\n",
    "            else:\n",
    "                frozen_params += param.numel()\n",
    "        \n",
    "        print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "        print(f\"  - Backbone: {backbone_trainable:,}\")\n",
    "        print(f\"  - Embedding Head: {head_trainable:,}\")\n",
    "        print(f\"Frozen parameters: {frozen_params:,} ({100 * frozen_params / total_params:.2f}%)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding_head(features)\n",
    "        # L2 normalize embeddings\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5946eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training function with layer freezing and differential learning rates\n",
    "def train_embedding_model(\n",
    "    reference_dir='data/reference_images',\n",
    "    backbone='efficientnet_v2_s',\n",
    "    embedding_dim=512,\n",
    "    batch_size=32,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    backbone_lr_multiplier=0.1,  # Learning rate multiplier for backbone (if unfrozen)\n",
    "    margin=0.5,\n",
    "    save_path='models/product_embedding_model.pth',\n",
    "    resume_from_checkpoint=None,\n",
    "    resume_from_best=False,\n",
    "    freeze_backbone=True,\n",
    "    unfreeze_last_n_layers=0  # Number of backbone layers to unfreeze\n",
    "):\n",
    "    \"\"\"\n",
    "    Train embedding model with configurable backbone freezing\n",
    "    \n",
    "    Args:\n",
    "        freeze_backbone: If True, freeze backbone (except last N layers if specified)\n",
    "        unfreeze_last_n_layers: Number of last backbone layers to unfreeze (0 = all frozen)\n",
    "        backbone_lr_multiplier: Learning rate multiplier for backbone (typically 0.1-0.01)\n",
    "    \"\"\"\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Determine model paths\n",
    "    best_model_path = save_path\n",
    "    last_model_path = save_path.replace('.pth', '_last.pth')\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = ProductDataset(reference_dir, augment=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Model with layer freezing\n",
    "    model = ProductEmbeddingModel(\n",
    "        backbone, \n",
    "        embedding_dim,\n",
    "        freeze_backbone=freeze_backbone,\n",
    "        unfreeze_last_n_layers=unfreeze_last_n_layers\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print parameter summary\n",
    "    model.print_trainable_params()\n",
    "    \n",
    "    # Loss and optimizer with differential learning rates\n",
    "    criterion = TripletLoss(margin=margin)\n",
    "    \n",
    "    # Separate learning rates for backbone and head\n",
    "    if unfreeze_last_n_layers > 0:\n",
    "        # Different learning rates for backbone and head\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'backbone' in name:\n",
    "                    backbone_params.append(param)\n",
    "                else:\n",
    "                    head_params.append(param)\n",
    "        \n",
    "        if backbone_params:\n",
    "            optimizer = optim.Adam([\n",
    "                {'params': backbone_params, 'lr': learning_rate * backbone_lr_multiplier},\n",
    "                {'params': head_params, 'lr': learning_rate}\n",
    "            ])\n",
    "            print(f\"Using differential learning rates:\")\n",
    "            print(f\"  Backbone: {learning_rate * backbone_lr_multiplier:.6f}\")\n",
    "            print(f\"  Head: {learning_rate:.6f}\\n\")\n",
    "        else:\n",
    "            # Only head params (backbone fully frozen)\n",
    "            optimizer = optim.Adam(head_params, lr=learning_rate)\n",
    "            print(f\"Using single learning rate: {learning_rate:.6f}\\n\")\n",
    "    else:\n",
    "        # Single learning rate (backbone frozen or all trainable)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Using single learning rate: {learning_rate:.6f}\\n\")\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    # Checkpoint resuming logic\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Determine which checkpoint to load\n",
    "    if resume_from_checkpoint is not None:\n",
    "        checkpoint_path = resume_from_checkpoint\n",
    "        checkpoint_type = \"explicit\"\n",
    "    elif resume_from_best and Path(best_model_path).exists():\n",
    "        checkpoint_path = best_model_path\n",
    "        checkpoint_type = \"best\"\n",
    "    elif Path(last_model_path).exists():\n",
    "        checkpoint_path = last_model_path\n",
    "        checkpoint_type = \"last\"\n",
    "    else:\n",
    "        checkpoint_path = None\n",
    "        checkpoint_type = None\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        print(f\"\\nFound {checkpoint_type} checkpoint at {checkpoint_path}\")\n",
    "        print(\"Loading checkpoint to resume training...\")\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer state (if available)\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"Loaded optimizer state\")\n",
    "        \n",
    "        # Load scheduler state (if available)\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(\"Loaded scheduler state\")\n",
    "        \n",
    "        # Resume from saved epoch\n",
    "        if 'epoch' in checkpoint:\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "        \n",
    "        # Resume best loss\n",
    "        if 'loss' in checkpoint:\n",
    "            best_loss = checkpoint['loss']\n",
    "            print(f\"Previous best loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Verify architecture matches\n",
    "        if checkpoint.get('backbone') != backbone:\n",
    "            print(f\"Warning: Backbone mismatch! Checkpoint: {checkpoint.get('backbone')}, Current: {backbone}\")\n",
    "        if checkpoint.get('embedding_dim') != embedding_dim:\n",
    "            print(f\"Warning: Embedding dim mismatch! Checkpoint: {checkpoint.get('embedding_dim')}, Current: {embedding_dim}\")\n",
    "        \n",
    "        print(\"Checkpoint loaded successfully!\\n\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found. Starting training from scratch.\\n\")\n",
    "    \n",
    "    # Triplet sampler\n",
    "    triplet_sampler = TripletSampler(train_dataset)\n",
    "    \n",
    "    # Helper function to save checkpoint\n",
    "    def save_checkpoint(epoch, loss, val_loss=None, is_best=False, is_last=False):\n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'backbone': backbone,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'num_products': train_dataset.num_products,\n",
    "            'product_to_idx': train_dataset.product_to_idx,\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'learning_rate': scheduler.get_last_lr()[0],\n",
    "            'freeze_backbone': freeze_backbone,\n",
    "            'unfreeze_last_n_layers': unfreeze_last_n_layers\n",
    "        }\n",
    "        if val_loss is not None:\n",
    "            checkpoint_data['val_loss'] = val_loss\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save(checkpoint_data, best_model_path)\n",
    "            print(f\"Saved best model (loss: {loss:.4f})\")\n",
    "        \n",
    "        if is_last:\n",
    "            torch.save(checkpoint_data, last_model_path)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_images, batch_labels, batch_product_ids in progress_bar:\n",
    "            # Sample triplets\n",
    "            anchor_indices = []\n",
    "            positive_indices = []\n",
    "            negative_indices = []\n",
    "            \n",
    "            for _ in range(batch_size):\n",
    "                a, p, n = triplet_sampler.sample_triplet()\n",
    "                anchor_indices.append(a)\n",
    "                positive_indices.append(p)\n",
    "                negative_indices.append(n)\n",
    "            \n",
    "            # Get images\n",
    "            anchor_images = torch.stack([train_dataset[i][0] for i in anchor_indices]).to(device)\n",
    "            positive_images = torch.stack([train_dataset[i][0] for i in positive_indices]).to(device)\n",
    "            negative_images = torch.stack([train_dataset[i][0] for i in negative_indices]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            anchor_emb = model(anchor_images)\n",
    "            positive_emb = model(positive_images)\n",
    "            negative_emb = model(negative_images)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        is_best = avg_loss < best_loss\n",
    "        if is_best:\n",
    "            best_loss = avg_loss\n",
    "        \n",
    "        # Save checkpoints\n",
    "        save_checkpoint(epoch, avg_loss, is_best=is_best, is_last=True)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - LR: {current_lr:.6f}\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "    print(f\"Last model saved at: {last_model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16fc3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 208 images from 68 products\n",
      "Backbone: Unfroze last 2 feature blocks (out of 8)\n",
      "\n",
      "============================================================\n",
      "MODEL PARAMETER SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total parameters: 22,540,880\n",
      "Trainable parameters: 17,255,464 (76.55%)\n",
      "  - Backbone: 14,892,072\n",
      "  - Embedding Head: 2,363,392\n",
      "Frozen parameters: 5,285,416 (23.45%)\n",
      "============================================================\n",
      "\n",
      "Using differential learning rates:\n",
      "  Backbone: 0.000100\n",
      "  Head: 0.001000\n",
      "\n",
      "No checkpoint found. Starting training from scratch.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|| 13/13 [00:50<00:00,  3.87s/it, loss=0.1253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.2334)\n",
      "Epoch 1/20 - Loss: 0.2334 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|| 13/13 [01:01<00:00,  4.76s/it, loss=0.0277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0810)\n",
      "Epoch 2/20 - Loss: 0.0810 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|| 13/13 [01:01<00:00,  4.77s/it, loss=0.0374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0474)\n",
      "Epoch 3/20 - Loss: 0.0474 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|| 13/13 [01:06<00:00,  5.15s/it, loss=0.0102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0296)\n",
      "Epoch 4/20 - Loss: 0.0296 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|| 13/13 [01:12<00:00,  5.60s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 0.0307 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|| 13/13 [01:45<00:00,  8.11s/it, loss=0.0456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0210)\n",
      "Epoch 6/20 - Loss: 0.0210 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|| 13/13 [01:22<00:00,  6.32s/it, loss=0.0086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 0.0260 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|| 13/13 [01:09<00:00,  5.37s/it, loss=0.0076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 0.0265 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|| 13/13 [01:10<00:00,  5.44s/it, loss=0.0210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 0.0223 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|| 13/13 [01:10<00:00,  5.43s/it, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0132)\n",
      "Epoch 10/20 - Loss: 0.0132 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|| 13/13 [01:11<00:00,  5.47s/it, loss=0.0042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Loss: 0.0182 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|| 13/13 [01:09<00:00,  5.34s/it, loss=0.0086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Loss: 0.0152 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|| 13/13 [01:10<00:00,  5.39s/it, loss=0.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0130)\n",
      "Epoch 13/20 - Loss: 0.0130 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|| 13/13 [01:10<00:00,  5.42s/it, loss=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Loss: 0.0132 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|| 13/13 [01:10<00:00,  5.41s/it, loss=0.0094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Loss: 0.0132 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|| 13/13 [01:10<00:00,  5.41s/it, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Loss: 0.0191 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|| 13/13 [01:09<00:00,  5.36s/it, loss=0.0346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0109)\n",
      "Epoch 17/20 - Loss: 0.0109 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|| 13/13 [01:10<00:00,  5.41s/it, loss=0.0065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Loss: 0.0161 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|| 13/13 [01:09<00:00,  5.38s/it, loss=0.0024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (loss: 0.0057)\n",
      "Epoch 19/20 - Loss: 0.0057 - LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|| 13/13 [01:09<00:00,  5.37s/it, loss=0.0210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Loss: 0.0178 - LR: 0.000050\n",
      "\n",
      "Training complete! Best loss: 0.0057\n",
      "Best model saved at: models/product_embedding_model_unfrozen2.pth\n",
      "Last model saved at: models/product_embedding_model_unfrozen2_last.pth\n"
     ]
    }
   ],
   "source": [
    "model = train_embedding_model(\n",
    "    reference_dir='data/reference_images',\n",
    "    backbone='efficientnet_v2_s',\n",
    "    embedding_dim=1024,\n",
    "    batch_size=16,\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.001,\n",
    "    backbone_lr_multiplier=0.1,  # 10x lower LR for backbone\n",
    "    margin=0.5,\n",
    "    freeze_backbone=True,\n",
    "    unfreeze_last_n_layers=2,  # Unfreeze last 2 blocks\n",
    "    save_path='models/product_embedding_model_unfrozen2.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0a2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
