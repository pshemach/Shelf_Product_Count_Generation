{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f274a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544f1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc0f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class ProductEmbeddingMatcher:\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.model = CLIPModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_embedding(self, image):\n",
    "        \"\"\"Extract embedding from product image\"\"\"\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model.get_image_features(**inputs)\n",
    "        return embedding.numpy().astype('float32')\n",
    "    \n",
    "    def build_faiss_index(self, reference_embeddings):\n",
    "        \"\"\"Build FAISS index for fast similarity search\"\"\"\n",
    "        dimension = reference_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "        index.add(reference_embeddings)\n",
    "        return index\n",
    "    \n",
    "    def find_matches(self, query_embedding, index, top_k=5):\n",
    "        \"\"\"Find top-k similar products\"\"\"\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18b6bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize CLIP\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()\n",
    "\n",
    "def get_embedding(image):\n",
    "    \"\"\"Extract normalized embedding from image\"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        # CLIP already normalizes, but ensure it\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "    return embedding.numpy().astype('float32')\n",
    "\n",
    "# Build FAISS index with Inner Product (cosine similarity)\n",
    "def build_cosine_index(embeddings):\n",
    "    \"\"\"Build FAISS index for cosine similarity search\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    # IndexFlatIP = Inner Product (dot product) = cosine for normalized vectors\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Search with cosine similarity\n",
    "def find_similar_products(query_embedding, index, product_ids, top_k=5):\n",
    "    \"\"\"Find top-k similar products using cosine similarity\"\"\"\n",
    "    # Search returns dot product scores (higher = more similar)\n",
    "    scores, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        # Score is already cosine similarity (0-1 range for normalized embeddings)\n",
    "        results.append({\n",
    "            'product_id': product_ids[idx],\n",
    "            'similarity_score': float(score),  # This is cosine similarity\n",
    "            'rank': i + 1\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1782f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 product folders\n",
      "  Product 1000: 3 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product 1001: 4 images\n",
      "  Product 1002: 4 images\n",
      "  Product 1003: 3 images\n",
      "  Product 1004: 5 images\n",
      "  Product 1005: 2 images\n",
      "  Product 1006: 5 images\n",
      "  Product 1007: 3 images\n",
      "  Product 1008: 2 images\n",
      "  Product 1009: 3 images\n",
      "  Product 1010: 2 images\n",
      "  Product 1011: 2 images\n",
      "  Product 1012: 3 images\n",
      "  Product 1013: 2 images\n",
      "  Product 1014: 2 images\n",
      "  Product 1015: 2 images\n",
      "  Product 1016: 2 images\n",
      "  Product 1017: 2 images\n",
      "  Product 1018: 2 images\n",
      "  Product 1019: 2 images\n",
      "  Product 1020: 2 images\n",
      "  Product 15785: 3 images\n",
      "  Product 15827: 3 images\n",
      "  Product 15829: 6 images\n",
      "  Product 15832: 5 images\n",
      "  Product 58: 8 images\n",
      "  Product 59: 4 images\n",
      "  Product 60: 3 images\n",
      "  Product 667: 5 images\n",
      "  Product 668: 3 images\n",
      "  Product 669: 4 images\n",
      "  Product 670: 4 images\n",
      "  Product 671: 2 images\n",
      "  Product 672: 7 images\n",
      "  Product 673: 3 images\n",
      "  Product 674: 2 images\n",
      "  Product 675: 3 images\n",
      "  Product 677: 4 images\n",
      "  Product 74: 4 images\n",
      "  Product 75: 2 images\n",
      "  Product 76: 4 images\n",
      "  Product 77: 7 images\n",
      "  Product 78: 6 images\n",
      "  Product 79: 5 images\n",
      "  Product 80: 2 images\n",
      "  Product 89: 2 images\n",
      "  Product 9: 2 images\n",
      "  Product 90: 2 images\n",
      "  Product 91: 3 images\n",
      "\n",
      "Total embeddings: 165\n",
      "Embedding dimension: 512\n",
      "\n",
      "FAISS index built with 165 vectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_reference_images_and_ids(reference_dir='data/reference_images'):\n",
    "    \"\"\"\n",
    "    Load all reference images and create product_id mapping.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array of all image embeddings\n",
    "        product_ids: list of product_ids corresponding to each embedding\n",
    "        image_paths: list of image paths for debugging\n",
    "    \"\"\"\n",
    "    reference_path = Path(reference_dir)\n",
    "    embeddings_list = []\n",
    "    product_ids_list = []\n",
    "    image_paths_list = []\n",
    "    \n",
    "    # Get all product folders (1000, 1001, 1002, etc.)\n",
    "    product_folders = sorted([d for d in reference_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    print(f\"Found {len(product_folders)} product folders\")\n",
    "    \n",
    "    for product_folder in product_folders:\n",
    "        product_id = product_folder.name  # e.g., \"1000\"\n",
    "        \n",
    "        # Get all images in this product folder\n",
    "        image_files = sorted(product_folder.glob('*.jpg')) + sorted(product_folder.glob('*.jpeg')) + sorted(product_folder.glob('*.png'))\n",
    "        \n",
    "        print(f\"  Product {product_id}: {len(image_files)} images\")\n",
    "        \n",
    "        for image_path in image_files:\n",
    "            try:\n",
    "                # Load image\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # Get embedding\n",
    "                embedding = get_embedding(image)\n",
    "                \n",
    "                embeddings_list.append(embedding)\n",
    "                product_ids_list.append(product_id)\n",
    "                image_paths_list.append(str(image_path))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading {image_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    \n",
    "    print(f\"\\nTotal embeddings: {embeddings.shape[0]}\")\n",
    "    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "    \n",
    "    return embeddings, product_ids_list, image_paths_list\n",
    "\n",
    "# Load all reference images\n",
    "reference_embeddings, product_ids, image_paths = load_reference_images_and_ids()\n",
    "\n",
    "# Build FAISS index\n",
    "index = build_cosine_index(reference_embeddings)\n",
    "\n",
    "print(f\"\\nFAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57a95dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "query_embedding = get_embedding('data/test_images/cropped_image_3.jpg')\n",
    "len(query_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b57f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = find_similar_products(query_embedding, index, product_ids, top_k=5)\n",
    "\n",
    "# Filter by threshold (e.g., 0.75 = 75% similarity)\n",
    "matches = [r for r in results if r['similarity_score'] > 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "deb0490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_id': '1002', 'similarity_score': 0.8852260708808899, 'rank': 1},\n",
       " {'product_id': '1003', 'similarity_score': 0.8810661435127258, 'rank': 2},\n",
       " {'product_id': '1003', 'similarity_score': 0.8785508871078491, 'rank': 3},\n",
       " {'product_id': '1005', 'similarity_score': 0.867941677570343, 'rank': 4},\n",
       " {'product_id': '1002', 'similarity_score': 0.8673974275588989, 'rank': 5}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047bca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a86d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
