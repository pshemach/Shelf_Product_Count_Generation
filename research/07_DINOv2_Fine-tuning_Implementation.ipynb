{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ae323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive - University of Moratuwa\\Desktop\\E-Vision-Projects\\Shelf_Product_Count_Generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset for DINOv2\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, reference_dir='data/reference_images', \n",
    "                 image_size=224, augment=True):\n",
    "        self.reference_dir = Path(reference_dir)\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load all images with their product IDs\n",
    "        self.images = []\n",
    "        self.product_ids = []\n",
    "        self.product_to_images = {}\n",
    "        \n",
    "        product_folders = sorted([d for d in self.reference_dir.iterdir() if d.is_dir()])\n",
    "        \n",
    "        for product_folder in product_folders:\n",
    "            product_id = product_folder.name\n",
    "            image_files = sorted(product_folder.glob('*.jpg')) + \\\n",
    "                         sorted(product_folder.glob('*.jpeg')) + \\\n",
    "                         sorted(product_folder.glob('*.png'))\n",
    "            \n",
    "            self.product_to_images[product_id] = []\n",
    "            \n",
    "            for image_path in image_files:\n",
    "                self.images.append(str(image_path))\n",
    "                self.product_ids.append(product_id)\n",
    "                self.product_to_images[product_id].append(len(self.images) - 1)\n",
    "        \n",
    "        # Create product ID to integer mapping\n",
    "        unique_products = sorted(set(self.product_ids))\n",
    "        self.product_to_idx = {pid: idx for idx, pid in enumerate(unique_products)}\n",
    "        self.idx_to_product = {idx: pid for pid, idx in self.product_to_idx.items()}\n",
    "        self.num_products = len(unique_products)\n",
    "        \n",
    "        # DINOv2 uses specific preprocessing\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {self.num_products} products\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        product_id = self.product_ids[idx]\n",
    "        product_idx = self.product_to_idx[product_id]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, product_idx, product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05dab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DINOv2 Model with Fine-tuning Head\n",
    "class FineTunedDINOv2(nn.Module):\n",
    "    def __init__(self, model_name='facebook/dinov2-base', embedding_dim=512, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained DINOv2\n",
    "        self.dinov2 = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Get embedding dimension from DINOv2\n",
    "        if 'base' in model_name:\n",
    "            dinov2_dim = 768\n",
    "        elif 'small' in model_name:\n",
    "            dinov2_dim = 384\n",
    "        elif 'large' in model_name:\n",
    "            dinov2_dim = 1024\n",
    "        else:\n",
    "            dinov2_dim = 768  # default\n",
    "        \n",
    "        # Freeze backbone if needed (for faster training)\n",
    "        if freeze_backbone:\n",
    "            for param in self.dinov2.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Fine-tuning head (projection layer)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(dinov2_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Forward pass through DINOv2\n",
    "        pixel_values: preprocessed images (batch_size, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        # DINOv2 forward pass\n",
    "        outputs = self.dinov2(pixel_values=pixel_values)\n",
    "        \n",
    "        # Get CLS token (first token) - this is the image embedding\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # (batch_size, dinov2_dim)\n",
    "        \n",
    "        # Project to desired embedding dimension\n",
    "        embedding = self.projection_head(cls_token)\n",
    "        \n",
    "        # L2 normalize\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Triplet Loss for Metric Learning\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"\n",
    "        anchor: embedding of anchor image\n",
    "        positive: embedding of positive (same product) image\n",
    "        negative: embedding of negative (different product) image\n",
    "        \"\"\"\n",
    "        distance_positive = nn.functional.pairwise_distance(anchor, positive)\n",
    "        distance_negative = nn.functional.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return loss.mean()\n",
    "\n",
    "# 4. Triplet Sampler\n",
    "class TripletSampler:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.product_to_indices = {}\n",
    "        \n",
    "        for idx, product_id in enumerate(dataset.product_ids):\n",
    "            if product_id not in self.product_to_indices:\n",
    "                self.product_to_indices[product_id] = []\n",
    "            self.product_to_indices[product_id].append(idx)\n",
    "    \n",
    "    def sample_triplet(self):\n",
    "        \"\"\"Sample anchor, positive, negative\"\"\"\n",
    "        # Random anchor product\n",
    "        anchor_product = random.choice(list(self.product_to_indices.keys()))\n",
    "        \n",
    "        # Need at least 2 images for this product\n",
    "        if len(self.product_to_indices[anchor_product]) < 2:\n",
    "            # If only one image, duplicate it\n",
    "            anchor_idx = positive_idx = self.product_to_indices[anchor_product][0]\n",
    "        else:\n",
    "            anchor_idx, positive_idx = random.sample(\n",
    "                self.product_to_indices[anchor_product], 2\n",
    "            )\n",
    "        \n",
    "        # Random negative product (different from anchor)\n",
    "        negative_product = random.choice([\n",
    "            p for p in self.product_to_indices.keys() \n",
    "            if p != anchor_product\n",
    "        ])\n",
    "        negative_idx = random.choice(self.product_to_indices[negative_product])\n",
    "        \n",
    "        return anchor_idx, positive_idx, negative_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a5ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training Function\n",
    "def train_dinov2_model(\n",
    "    reference_dir='data/reference_images',\n",
    "    model_name='facebook/dinov2-base',\n",
    "    embedding_dim=512,\n",
    "    batch_size=16,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    margin=0.5,\n",
    "    freeze_backbone=False,\n",
    "    save_path='models/dinov2_finetuned.pth'\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Windows compatibility: set num_workers to 0\n",
    "    num_workers = 0 if sys.platform == 'win32' else 4\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = ProductDataset(reference_dir, augment=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,  # 0 for Windows, 4 for Linux/Mac\n",
    "        pin_memory=True if device.type == 'cuda' and num_workers > 0 else False,\n",
    "        persistent_workers=False  # Disable persistent workers for Windows\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = FineTunedDINOv2(\n",
    "        model_name=model_name,\n",
    "        embedding_dim=embedding_dim,\n",
    "        freeze_backbone=freeze_backbone\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = TripletLoss(margin=margin)\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Triplet sampler\n",
    "    triplet_sampler = TripletSampler(train_dataset)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_images, batch_labels, batch_product_ids in progress_bar:\n",
    "            # Sample triplets\n",
    "            anchor_indices = []\n",
    "            positive_indices = []\n",
    "            negative_indices = []\n",
    "            \n",
    "            for _ in range(batch_size):\n",
    "                a, p, n = triplet_sampler.sample_triplet()\n",
    "                anchor_indices.append(a)\n",
    "                positive_indices.append(p)\n",
    "                negative_indices.append(n)\n",
    "            \n",
    "            # Get images - handle errors gracefully\n",
    "            try:\n",
    "                anchor_images = torch.stack([train_dataset[i][0] for i in anchor_indices]).to(device)\n",
    "                positive_images = torch.stack([train_dataset[i][0] for i in positive_indices]).to(device)\n",
    "                negative_images = torch.stack([train_dataset[i][0] for i in negative_indices]).to(device)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading images: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Forward pass\n",
    "            try:\n",
    "                anchor_emb = model(anchor_images)\n",
    "                positive_emb = model(positive_images)\n",
    "                negative_emb = model(negative_images)\n",
    "                \n",
    "                # Loss\n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            except Exception as e:\n",
    "                print(f\"Error in forward/backward pass: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if num_batches == 0:\n",
    "            print(f\"Warning: No batches processed in epoch {epoch+1}\")\n",
    "            continue\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_name': model_name,\n",
    "                'embedding_dim': embedding_dim,\n",
    "                'num_products': train_dataset.num_products,\n",
    "                'product_to_idx': train_dataset.product_to_idx,\n",
    "                'epoch': epoch,\n",
    "                'loss': avg_loss\n",
    "            }, save_path)\n",
    "            print(f\"✓ Saved best model (loss: {avg_loss:.4f})\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# 6. Load Trained Model\n",
    "def load_trained_dinov2(model_path='models/dinov2_finetuned.pth', device='cuda'):\n",
    "    \"\"\"Load trained DINOv2 model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    model = FineTunedDINOv2(\n",
    "        model_name=checkpoint['model_name'],\n",
    "        embedding_dim=checkpoint['embedding_dim'],\n",
    "        freeze_backbone=False\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, checkpoint['product_to_idx']\n",
    "\n",
    "# 7. Extract Embeddings with Trained Model\n",
    "def get_embedding_dinov2_trained(model, image, device='cuda'):\n",
    "    \"\"\"Extract embedding using trained DINOv2\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert('RGB')\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "    \n",
    "    return embedding.cpu().numpy().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68fc32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 166 images from 49 products\n",
      "Trainable parameters: 87,893,760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 21/21 [07:17<00:00, 20.84s/it, loss=0.4016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.4513 - LR: 0.000091\n",
      "✓ Saved best model (loss: 0.4513)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 21/21 [07:39<00:00, 21.88s/it, loss=0.3107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 0.4023 - LR: 0.000066\n",
      "✓ Saved best model (loss: 0.4023)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 21/21 [08:12<00:00, 23.44s/it, loss=0.3757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 0.3787 - LR: 0.000035\n",
      "✓ Saved best model (loss: 0.3787)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 21/21 [07:10<00:00, 20.52s/it, loss=0.1203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.3075 - LR: 0.000010\n",
      "✓ Saved best model (loss: 0.3075)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 21/21 [06:50<00:00, 19.55s/it, loss=0.2303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 0.2295 - LR: 0.000001\n",
      "✓ Saved best model (loss: 0.2295)\n",
      "\n",
      "Training complete! Best loss: 0.2295\n"
     ]
    }
   ],
   "source": [
    "# 8. Usage Example\n",
    "model = train_dinov2_model(\n",
    "        reference_dir='data/reference_images',\n",
    "        model_name='facebook/dinov2-base',  # or 'facebook/dinov2-small' for faster training\n",
    "        embedding_dim=512,\n",
    "        batch_size=8,  # Start small, increase if you have GPU memory\n",
    "        num_epochs=5,\n",
    "        learning_rate=0.0001,\n",
    "        margin=0.5,\n",
    "        freeze_backbone=False  # Set True for faster training, False for better accuracy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1528e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
